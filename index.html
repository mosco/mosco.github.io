<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="">
    <meta name="author" content="">

    <title>Amit Moscovich - Academic homepage</title>

    <!-- Bootstrap Core CSS -->
    <link href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" rel="stylesheet">

    <!-- Custom Fonts -->
    <link href="vendor/font-awesome/css/font-awesome.min.css" rel="stylesheet" type="text/css">
    <link rel="stylesheet" href="vendor/font-awesome/css/academicons.css"/>
    <link href="https://fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic" rel="stylesheet" type="text/css">
    <link href="https://fonts.googleapis.com/css?family=Montserrat:400,700" rel="stylesheet" type="text/css">

    <!-- Theme CSS -->
    <link href="css/grayscale.css" rel="stylesheet">

    <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->

</head>

<body id="page-top" data-spy="scroll" data-target=".navbar-fixed-top">

    <!-- Navigation -->
    <nav class="navbar navbar-custom navbar-fixed-top" role="navigation">
        <div class="container">
            <div class="navbar-header">
                <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-main-collapse">
                    Menu <i class="fa fa-bars"></i>
                </button>
                <a class="navbar-brand page-scroll" href="#page-top">
                    <i class="fa fa-play-circle"></i> <span class="light">Home</span>
                </a>
            </div>
 
            <!-- Collect the nav links, forms, and other content for toggling -->
            <div class="collapse navbar-collapse navbar-right navbar-main-collapse">
                <ul class="nav navbar-nav">
                    <!-- Hidden li included to remove active class from about link when scrolled up past about section -->
                    <li class="hidden">
                        <a href="#page-top"></a>
                    </li>
                    <li>
                        <a class="page-scroll" href="#about">About</a>
                    </li>
                    <li>
                        <a class="page-scroll" href="#publications">Publications</a>
                    </li>
                    <li>
                        <a class="page-scroll" href="#research">Research</a>
                    </li>
                </ul>
            </div>
            <!-- /.navbar-collapse -->
        </div>
        <!-- /.container -->
    </nav>

    <!-- Intro Header -->
    <header class="intro">
        <div class="intro-body">
            <div class="container">
                <div class="row">
                    <div class="col-md-8 col-md-offset-2">
                        <h1 class="brand-heading">Amit Moscovich</h1>
                        <p class="intro-text">Academic homepage</p>
                        <a href="#about" class="btn btn-circle page-scroll">
                            <i class="fa fa-angle-double-down animated"></i>
                        </a>
                    </div>
                </div>
            </div>
        </div>
    </header>

    <!-- About Section -->
    <section id="about" class="container content-section">
        <div class="row">
            <div class="col-lg-12 col-lg-offset-0">

                <h3>*** On the job market for tenure-track positions ***</h3>
                <br>
                <br>
                <br>
                <h1 name="about">About</h1>

                <div>
                    <p>
                    Postdoctoral research associate at <a href="https://www.princeton.edu">Princeton University's</a> <a href="https://www.pacm.princeton.edu">Program in Applied and Computational Mathematics</a>, working with <a href="https://web.math.princeton.edu/~amits/">Prof. Amit Singer</a>.

                    Before that I spent a year as a postdoc in Tel-Aviv University working with <a href="http://www.tau.ac.il/~saharon/">Prof. Saharon Rosset</a>. I did my Ph.D. at the department of <a href="https://www.weizmann.ac.il/math/">Computer Science and Applied Mathematics</a>
                        at the <a href="https://www.weizmann.ac.il/">Weizmann Institute of Science</a> in Israel,
                        where <a href="http://www.wisdom.weizmann.ac.il/~/nadler/">Prof. Boaz Nadler</a> was my doctoral advisor.
                    </p>
                    <p>
                        My research is focused on developing methodology for data analysis. I am particularly interested in problems that are at the intersection of statistics, applied mathematics and efficient computation.
                    </p>
                    <p>Address: 215 Fine Hall, Princeton NJ, USA.</p>
                </div>

                <div style="margin-left: 0; width: 85%;">
                    <a href="mailto:amit@moscovich.org" class="btn btn-default btn-lg"><i class="fa fa-envelope-square fa-fw"></i> <span class="network-name">email</span></a>
                    <a href="https://scholar.google.co.il/citations?user=978bqAwAAAAJ" class="btn btn-default btn-lg"><i class="ai ai-google-scholar-square"></i> <span class="network-name">Google Scholar</span></a>
                    <a href="https://www.researchgate.net/profile/Amit_Moscovich" class="btn btn-default btn-lg"><i class="ai ai-researchgate-square"></i> <span class="network-name">Research Gate</span></a>
                    <a href="https://github.com/mosco" class="btn btn-default btn-lg"><i class="fa fa-github fa-fw"></i> <span class="network-name">Github</span></a>
                    <a href="https://twitter.com/amitmoscovich" class="btn btn-default btn-lg"><i class="fa fa-twitter"></i> <span class="network-name">Twitter</span></a>
                </div>

                <br>
                <br>
                <br>
                <br>
                <img src="img/me.jpeg" style="width: 35%;">
                <img src="img/family-picture-1.jpeg" style="width: 62.2%;">
            </div>
        </div>
    </section>

    <section id="publications" class="content-section text-center">
        <div class="transition-section1">
            <div class="container">
                <div class="col-lg-12 col-lg-offset-0">
                    <br>
                    <br>
                    <br>
                    <h1>Publications</h1>
                </div>
            </div>
        </div>
    </section>

    <section class="container content-section">
        <div class="row">
            <div class="col-lg-12 col-lg-offset-0">
                <p>
                    Amit Moscovich.<br>
                    <i>Fast calculation of p-values for one-sided Kolmogorov-Smirnov type statistics.</i><br>
                    Submitted.<br>
                    [
                        <a href="https://arxiv.org/abs/2009.04954">paper</a>
                        |
                        <a href="crossprob/benchmarks.tar.gz">code</a>
                    ]
                </p>

                <p>
                    Amit Moscovich, Saharon Rosset.<br>
                    <i>On the cross-validation bias due to unsupervised pre-processing.</i><br>
                    Under revision for Journal of the Royal Statistical Society Series B (Statistical Methodology).<br>
                    [
                        <a href="https://arxiv.org/abs/1901.08974">paper</a>
                        |
                        <a href="http://github.com/mosco/unsupervised-preprocessing">code</a>
                    ]
                </p>
                
                <p>
                    Yaniv Tenzer, Amit Moscovich, Mary-Frances Dorn, Boaz Nadler, Clifford Spiegelman.<br>
                    <i>Beyond trees: classification with sparse pairwise dependencies.</i><br>
                    To appear in Journal of Machine Learning Research (JMLR).<br>
                    [
                        <a href="https://arxiv.org/abs/1806.01993">paper</a>
                    ]
                </p>


                <p>
                    Nathan Zelesko, Amit Moscovich, Joe Kileel, Amit Singer.<br>
                    <i>Earthmover-based manifold learning for analyzing molecular conformation spaces.</i><br>
                    IEEE 17th International Symposium on Biomedical Imaging (ISBI), 2020.<br>
                    [ 
                        <a href="https://ieeexplore.ieee.org/document/9098723">paper</a>
                        |
                        <a href="https://github.com/nathanzelesko/earthmover">code</a>
                    ]
                </p>

                <p>
                    Ye Zhou, Amit Moscovich, Tamir Bendory, Alberto Bartesaghi.<br>
                    <i>Unsupervised particle sorting for high-resolution single-particle cryo-EM.</i><br>
                    Inverse Problems, 2020.<br>
                    [
                        <a href="https://iopscience.iop.org/article/10.1088/1361-6420/ab5ec8">paper</a>
                    ]
                </p>

                <p>
                    Amit Moscovich, Amit Halevi, Joakim And&eacute;n, Amit Singer.<br>
                    <i>Cryo-EM reconstruction of continuous heterogeneity by Laplacian spectral volumes.</i><br>
                    Inverse Problems, 2020.<br>
                    [ 
                        <a href="https://iopscience.iop.org/article/10.1088/1361-6420/ab4f55">paper</a>
                        |
                        <a href="https://github.com/PrincetonUniversity/specvols">code</a>
                    ] 
                </p>

                <p>
                    Amit Moscovich, Ariel Jaffe, Boaz Nadler.<br>
                    <i>Minimax-optimal semi-supervised regression on unknown manifolds.</i><br>
                    Proceedings of the 20th International Conference on Artificial Intelligence and Statistics (AISTATS), 2017.<br>
                    [
                        <a href="http://proceedings.mlr.press/v54/moscovich17a/moscovich17a.pdf">paper</a>
                        |
                        <a href="http://proceedings.mlr.press/v54/moscovich17a/moscovich17a-supp.pdf">supplementary</a>
                        |
                        <a href="geodesic-knn/Minimax-optimal semi-supervised regression on unknown manifolds poster.pdf">poster</a>
                        |
                        <a href="geodesic-knn/geodesic-knn-slides-11.pdf">slides</a>
                    ]
                </p>

                <p>
                    Amit Moscovich, Boaz Nadler.<br>
                    <i>Fast calculation of boundary crossing probabilities for Poisson processes.</i><br>
                    Statistics &amp; Probability Letters, 2017.<br>
                    [
                        <a href="https://www.sciencedirect.com/science/article/abs/pii/S0167715216302802?via%3Dihub">paper</a>
                        |
                        <a href="crossprob/crossing_probability_poster_mlss2015.pdf">poster</a>
                    ]
                </p>

                <p>
                    Amit Moscovich, Boaz Nadler, Clifford Spiegelman.<br>
                    <i>On the exact Berk-Jones statistics and their p-value calculation.</i><br>
                    Electronic Journal of Statistics, 2016.<br>
                    [
                        <a href="https://doi.org/10.1214/16-EJS1172">paper</a>
                        |
                        <a href="http://doi.org/10.1214/16-EJS1172SUPP">supplementary</a>
                        |
                        <a href="berkjones/exact_bj_paper_produce_figures.zip">code</a>
                    ]
                </p>
            </div>
        </div>
    </section>

    <section id="research" class="content-section text-center">
        <div class="transition-section2">
            <div class="container">
                <div class="col-lg-12 col-lg-offset-0">
                    <br>
                    <br>
                    <br>
                    <h1>Research</h1>
                </div>
            </div>
        </div>
    </section>

    <section id="shapemanifolds" class="container content-section">
        <div class="row">
            <div class="col-lg-12 col-lg-offset-0">
                <h1>Learning shape manifolds</h1>
                <h4> Documents </h4>

                <p>
                    Nathan Zelesko, Amit Moscovich, Joe Kileel, Amit Singer.<br>
                    <i>Earthmover-based manifold learning for analyzing molecular conformation spaces.</i><br>
                    IEEE 17th International Symposium on Biomedical Imaging (ISBI), 2020.<br>
                    [ 
                        <a href="https://ieeexplore.ieee.org/document/9098723">paper</a>
                        |
                        <a href="https://github.com/nathanzelesko/earthmover">code</a>
                    ]
                </p>
                <p>
                    Amit Moscovich, Amit Halevi, Joakim And&eacute;n, Amit Singer.<br>
                    <i>Cryo-EM reconstruction of continuous heterogeneity by Laplacian spectral volumes.</i><br>
                    Inverse Problems, 2020.<br>
                    [ 
                        <a href="https://iopscience.iop.org/article/10.1088/1361-6420/ab4f55">paper</a>
                        |
                        <a href="https://github.com/PrincetonUniversity/specvols">code</a>
                    ] 
                </p>
            </div>
        </div>
    </section>
    <section id="crossprob" class="container content-section">
        <div class="row">
            <div class="col-lg-12 col-lg-offset-0">
                <h1>Boundary crossing</h1>
                <p>
                    Consider a one-dimensional homogeneous <a href="https://en.wikipedia.org/wiki/Poisson_point_process#Defined_on_the_real_line">Poisson process</a>.
                    Given arbitrary boundary functions from above and below, how can we compute the boundary crossing probability
                    of this process?
                    This problem has several applications in statistics, including the computation of exact p-values and power for supremum-based continuous goodness-of-fit statistics, such as Kolmogorov-Smirnov, Berk-Jones and others.
                </p>
                <img src="img/crossprob-illustration.svg" class="img-rounded" style="width: 70%; height: auto; margin-left: auto; margin-right: auto; display: block"><br><br>
                <p>
                    Despite more than 50 years of research, the most efficient practical methods to date compute this probability in O(n^3) time, where <b>n</b> is an upper bound on the boundary functions.
                    In this work, we discovered a simple and practical algorithm for this problem that solves it in O(n^2 log n) and a fast and numerically stable that solves the one-sided boundary crossing in O(n^2) time.
                </p>
                <h4> Documents </h4>
                
                <p>
                    Amit Moscovich.<br>
                    <i>Fast calculation of p-values for one-sided Kolmogorov-Smirnov type statistics.</i><br>
                    Submitted.<br>
                    [
                        <a href="https://arxiv.org/abs/2009.04954">paper</a>
                        |
                        <a href="crossprob/benchmarks.tar.gz">code</a>
                    ]
                </p>
                <p>
                    Amit Moscovich, Boaz Nadler.<br>
                    <i>Fast calculation of boundary crossing probabilities for Poisson processes.</i><br>
                    Statistics &amp; Probability Letters, 2017.<br>
                    [
                        <a href="https://www.sciencedirect.com/science/article/abs/pii/S0167715216302802?via%3Dihub">paper</a>
                        |
                        <a href="crossprob/crossing_probability_poster_mlss2015.pdf">poster</a>
                    ]
                </p>
                <p>
                    Amit Moscovich, Boaz Nadler, Clifford Spiegelman.<br>
                    <i>On the exact Berk-Jones statistics and their p-value calculation.</i><br>
                    Electronic Journal of Statistics, 2016.<br>
                    [
                        <a href="https://doi.org/10.1214/16-EJS1172">paper</a>
                        |
                        <a href="http://doi.org/10.1214/16-EJS1172SUPP">supplementary</a>
                        |
                        <a href="berkjones/exact_bj_paper_produce_figures.zip">code</a>
                    ]
                </p>
                <h4> Code </h4>
                <p>
                    A fast C++ implementation of the algorithms discussed in these papers:
                    <a href="https://github.com/mosco/crossing-probability">crossing-probability</a>
                </p>
            </div>
        </div>
    </section>
    <section id="geodesicnn" class="container content-section">
        <div class="row">
            <div class="col-lg-12 col-lg-offset-0">
                <h1 name="geodesicnn">Geodesic nearest neighbors</h1>
                <p>
                Consider nonparametric regression (e.g. <a href="https://en.wikipedia.org/wiki/Kernel_regression">Nadaraya-Watson</a>) in a high-dimensional metric space. In the general case, this typically requires an unreasonable number of labeled points due to the curse of dimensionality.
                We make two simplifying assumptions:
                    <ul>
                        <li><p><b>Manifold assumption:</b> The data points are supported on (or near) a low-dimensional manifold.</p></li>
                        <li><p><b>Semi-supervised data set:</b> In addition to the labeled points, we are given a <b>large</b> sample of unlabeled points</p></li>
                    </ul>
                </p>
                <p>
                    If there are many unlabeled points, then we can use them to estimate geodesic distances along the (unknown) manifold.
                    This is done by connecting pairs of close points by a weighted edge with weight equal to their euclidean distance.
                    Geodesic distances in the resulting graph approximate geodesic distances in the manifold.
                    We propose to use this idea to apply standard nonparametric methods, but using the manifold distances instead of the euclidean distances. Thereby avoiding the curse of dimensionality.
                </p>
                <img src="img/isomap.jpg" class="img-rounded" style="margin-left: auto; margin-right: auto; display: block">
                <p style="text-align: center">
                    <small>Figure shamelessly stolen from <a href="http://isomap.stanford.edu/">IsoMap homepage</a></small>
                </p>
                <p>
                    One way to think about manifold/graph-based semi-supervised methods, is to consider their Euclidean analogues.
                    Consider the following table:
                </p>

                <table class="table">
                    <thead>
                        <tr>
                            <th>Nonparametric regression method</th>
                            <th>Analogous approach for graphs/manifolds</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td>Regression using orthogonal functions</td>
                            <td><a href="https://stuff.mit.edu/afs/athena/course/9/9.520/www/spring08/Papers/Belkin-ML-04.pdf">Laplacian eigenvector regression</a></td>
                        </tr>
                        <tr>
                            <td>Wavelet regression</td>
                            <td><a href="https://web.stanford.edu/~gavish/documents/Haarlike_ICML_2010.pdf">Graph multiscale wavelet regression</a></td>
                        </tr>
                        <tr>
                            <td>Nadaraya-Watson / K-nearest neighbor</td>
                            <td><b>Geodesic nearest-neighbor</b></td>
                        </tr>
                    </tbody>
                </table>

                <p>
                    K nearest-neighbors and Nadaraya-watson regression are fundamental nonparametric regression method, but their manifold analogues have barely been studied.
                    This was the initial motivation for this project. Results:
                </p>
                <h4> Key results </h4>
                <p>
                    <ul>
                        <li><p><b>Minimax optimality:</b> given a sufficient number of unlabeled data points from an unknown manifold, the geodesic K nearest-neighbors regressor obtains the finite-sample minimax bound for a Lipschitz function on the manifold, as if it were completely specified.</p></li>
                        <li><p><b>Good performance:</b> on manifold-structured signals.
                        <li><p><b>Efficient computation:</b> regression and classification methods based on geodesic nearest neighbors can be efficiently computed, both for the transductive and the inductive cases of semi-supervised learning.
                    </ul>
                </p>
                <h4> Documents </h4>
                <p>
                    Amit Moscovich, Ariel Jaffe, Boaz Nadler.<br>
                    <i>Minimax-optimal semi-supervised regression on unknown manifolds.</i><br>
                    Proceedings of the 20th International Conference on Artificial Intelligence and Statistics (AISTATS), 2017.<br>
                    [
                        <a href="http://proceedings.mlr.press/v54/moscovich17a/moscovich17a.pdf">paper</a>
                        |
                        <a href="http://proceedings.mlr.press/v54/moscovich17a/moscovich17a-supp.pdf">supplementary</a>
                        |
                        <a href="geodesic-knn/Minimax-optimal semi-supervised regression on unknown manifolds poster.pdf">poster</a>
                        |
                        <a href="geodesic-knn/geodesic-knn-slides-11.pdf">slides</a>
                    ]
                </p>
                <img src="img/real_data_floor.png" class="img-rounded" style="width: 41.5%; margin-left: auto; margin-right: auto;">
                <img src="img/SimEnvironment.png" class="img-rounded" style="width: 52%; margin-left: auto; margin-right: auto;">
                <br>
                <br>
                <br>
                <br>
                <h4> Code </h4>
                <p>
                    Code for efficiently finding K-nearest-labeled vertices:
                    <a href="https://github.com/mosco/graphknn">Graph KNN Python module</a>
                </p>
            </div>
        </div>
    </section>

    <section id="exactbj" class="container content-section">
        <div class="row">
            <div class="col-lg-12 col-lg-offset-0">
                <h1 name="exactbj">The exact Berk-Jones statistics</h1>
                <p>
                This study started with an idea to modify the <a href="https://projecteuclid.org/euclid.aos/1085408492">Higher Criticism statistic</a> so as to improve its finite-sample behavior. After a lot of digging, this turned out to be a rediscovery of the M<sub>n</sub> goodness-of-fit statistic proposed by <a href="https://link.springer.com/article/10.1007/BF00533250">Berk and Jones in 1979</a>.
                <p>
                    In this work we present a new derivation of the exact Berk-Jones statistics, prove that they are asymptotically optimal for the detection of Sparse Mixtures and consistent. We also compare them to other goodness-of-fit statistics and present an efficient method to compute the p-values of the one-sided statistics.
                </p>
                <p>
                    Overall, I believe these results demonstrate that the exact Berk-Jones statistics are an interesting alternative to other goodness-of-fit statistics such as <a href="https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test">Kolmogorov-Smirnov</a> and <a href="https://en.wikipedia.org/wiki/Anderson%E2%80%93Darling_test">Anderson-Darling</a>, despite having received (much) less attention. They are particularly suited for the detection of alternatives that differ from the null hypothesis at the tails of the distribution. </p>
                <img src="img/rare_weak_misdetection_leader_n1000.png" class="img-rounded" style="width: 50%; margin-left: auto; margin-right: auto;">
                <img src="img/normal_scaling_ks_vs_ad_vs_mn_n100.svg" class="img-rounded" style="width: 47.4%; margin-left: auto; margin-right: auto;">
                <br>
                <br>
                <br>
                <br>
                <h4> Documents </h4>
                <p>
                    Amit Moscovich, Boaz Nadler, Clifford Spiegelman.<br>
                    <i>On the exact Berk-Jones statistics and their p-value calculation.</i><br>
                    Electronic Journal of Statistics, 2016.<br>
                    [
                        <a href="https://doi.org/10.1214/16-EJS1172">paper</a>
                        |
                        <a href="http://doi.org/10.1214/16-EJS1172SUPP">supplementary</a>
                        |
                        <a href="berkjones/exact_bj_paper_produce_figures.zip">code</a>
                    ]
                </p>
                <br>
                <br>
                <br>
                <br>
                <h4> Code </h4>
                <p>
                    <b>Code for computing exact Berk-Jones statistic:</b>
                    <a href="berkjones/exact_berk_jones.py">Python</a>, <a href="berkjones/exact_berk_jones.R">R</a>
                </p>
                <p>
                For computing the p-value of the M<sub>n</sub>, M<sub>n</sub><sup>+</sup> and M<sub>n</sub><sup>-</sup> statistics, see the <a class="page-scroll" href="#crossprob">Boundary Crossing</a> section.
                </p>
                <p>
                    Code that reproduces all of the figures in the paper: <a href="berkjones/exact_bj_paper_produce_figures.zip">exact_bj_paper_produce_figures.zip</a>
                </p>
            </div>
        </div>
    </section>


    <section id="forestdensity" class="container content-section">
        <div class="row">
            <div class="col-lg-12 col-lg-offset-0">
                <h1>Classification of sparse graphical models</h1>
                <p>
                    Consider the problem of classifying vectors into one of two classes, where both classes
                    follow different forest-structured undirected graphical models (also known as Markov random fields).
                </p>
                <img src="img/simforestplots.svg" class="img-rounded" style="width: 70%; height: auto;  margin-left: auto; margin-right: auto; display: block"><br><br>
                <p>
                    We show that the optimal classifier is linear after applying a particular oracle transformation of the feature vectors.
                    This transformation takes a feature vector <b>x</b> and returns the log of all of the univariate and bivariate densities of the two classes at <b>x</b>.

                    Based on this observation, we propose a classifier that is tailor-made for this scenario.
                    It first constructs an estimate of the oracle transformation, and then fits a linear SVM classifier
                    to the transformed data.
                </p>
                <p>
                    We prove convergence of the resulting classifier to an oracle SVM classifier
                    and give finite sample bounds on its excess risk.                 
                </p>
                <p>
                    Experiments with simulated and real data indicate that the resulting classifier
                    is competitive with several popular methods across a range of applications.
                    This is true even on datasets that do not follow the forest assumption.
                </p>
                <h4> Documents </h4>
                <p>
                    Yaniv Tenzer, Amit Moscovich, Mary-Frances Dorn, Boaz Nadler, Clifford Spiegelman.<br>
                    <i>Beyond trees: classification with sparse pairwise dependencies.</i><br>
                    Accepted to Journal of Machine Learning Research (JMLR).<br>
                    [
                        <a href="https://arxiv.org/abs/1806.01993">paper</a>
                    ]
                </p>
            </div>
        </div>
    </section>


    <!-- Footer -->
    <footer>
        <div class="container text-center">
            Page design based on the <a href="http://startbootstrap.com/template-overviews/grayscale/">Grayscale</a> bootstrap theme.
        </div>
    </footer>

    <!-- jQuery -->
    <script src="vendor/jquery/jquery.js"></script>

    <!-- Bootstrap Core JavaScript -->
    <script src="vendor/bootstrap/js/bootstrap.min.js"></script>

    <!-- Plugin JavaScript -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery-easing/1.3/jquery.easing.min.js"></script>

    <!-- Google Maps API Key - Use your own API key to enable the map feature. More information on the Google Maps API can be found at https://developers.google.com/maps/ -->
    <script type="text/javascript" src="https://maps.googleapis.com/maps/api/js?key=AIzaSyCRngKslUGJTlibkQ3FkfTxj3Xss1UlZDA&sensor=false"></script>

    <!-- Theme JavaScript -->
    <script src="js/grayscale.min.js"></script>

</body>

</html>
